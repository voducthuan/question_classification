{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3000 # size of vocabulary\n",
    "embedding_dim = 64\n",
    "max_length = 20\n",
    "training_portion = .80 # set ratio of train (80%) and validation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_questions = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5452\n",
      "5452\n"
     ]
    }
   ],
   "source": [
    "# Read data and remove stopword\n",
    "with open(\"data/train_5500.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        question = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            question = question.replace(token, ' ')\n",
    "            question = question.replace(' ', ' ')\n",
    "        list_of_questions.append(question)\n",
    "print(len(labels))\n",
    "print(len(list_of_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(list_of_questions) * training_portion)\n",
    "train_questions = list_of_questions[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "validation_questions = list_of_questions[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(train_questions)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(word_index.items())[0:100]) ## print out first 100 index of vocabulary\n",
    "train_sequences = tokenizer.texts_to_sequences(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2918, 1777, 691, 910]\n",
      "[1, 572, 479, 78, 911, 2919]\n",
      "[2, 9, 20, 480, 1224, 33, 245, 121]\n",
      "[1, 2920, 2921, 2922, 344, 21, 1225]\n",
      "[1, 295, 171, 345]\n",
      "[1, 1778, 1779, 912, 1226, 1780]\n",
      "[1, 72, 73, 3, 217, 692, 1781, 122]\n",
      "[1, 413, 913]\n",
      "[1, 1782, 2923]\n",
      "[6, 2924, 1783, 914, 1784, 12, 59, 172]\n",
      "[15, 2925, 2926, 80]\n",
      "[18, 1785, 1786, 272, 2927, 2928]\n",
      "[4, 12, 2929, 1227]\n",
      "[4, 114, 915]\n",
      "[1, 573, 2930, 2931, 916, 2932, 273, 1783]\n",
      "[1, 2933, 14, 13, 25, 218, 1787]\n",
      "[1, 2934, 917, 14, 13, 918, 1788]\n",
      "[2, 7, 2935, 919, 2936, 2937, 2938]\n",
      "[1, 414, 1789, 2939, 5]\n",
      "[1, 2940, 1228]\n",
      "[1, 130, 346, 49]\n",
      "[1, 1790, 920, 2941, 347]\n",
      "[6, 693, 50, 2942]\n",
      "[1, 3, 348, 574]\n",
      "[1, 40, 6, 921, 33]\n",
      "[1, 3, 100, 922, 45, 1791, 101]\n",
      "[4, 1792, 694, 923]\n",
      "[1, 81, 2943, 115, 82]\n",
      "[6, 415, 924, 1793, 481]\n",
      "[19, 28, 82, 2944, 2945, 575]\n",
      "[1, 195, 925, 55]\n",
      "[1, 2946, 1229]\n",
      "[2, 7, 695, 68, 696, 2947, 697, 576]\n",
      "[19, 42, 1794, 482, 53, 2948, 926, 246, 173, 5]\n",
      "[2, 7, 1795, 2949, 296, 347]\n",
      "[1, 1230, 416]\n",
      "[15, 2950, 58, 56]\n",
      "[29, 698, 2951, 1231, 1796]\n",
      "[8, 1232, 12, 2952, 417, 1797, 5, 51, 159]\n",
      "[1, 1798, 2953, 349, 41, 483, 350]\n",
      "[2, 9, 1799, 351, 927, 699]\n",
      "[1, 928, 2954, 2955, 2956, 352, 2957, 10, 2958]\n",
      "[2, 7, 24, 11, 1233, 174]\n",
      "[2, 7, 700, 353, 219, 1234, 418, 2959]\n",
      "[2, 20, 2960, 2961, 2962]\n",
      "[1, 1800, 577, 69, 1801, 1802, 1235, 143]\n",
      "[1, 1803, 578, 63, 2963, 131, 57, 2964, 2965, 2966]\n",
      "[1, 2967, 929, 930]\n",
      "[2, 701, 1804, 2968, 130]\n",
      "[1, 931, 2969, 78, 1805, 2970]\n"
     ]
    }
   ],
   "source": [
    "# First of 50 records in token form\n",
    "for i in range(50):\n",
    "    print(train_sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 2918 1777  691  910    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1  572  479   78  911 2919    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    9   20  480 1224   33  245  121    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 2920 2921 2922  344   21 1225    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  1 295 171 345   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   1 1778 1779  912 1226 1780    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1   72   73    3  217  692 1781  122    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  1 413 913   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   1 1782 2923    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   6 2924 1783  914 1784   12   59  172    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  15 2925 2926   80    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  18 1785 1786  272 2927 2928    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   4   12 2929 1227    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  4 114 915   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   1  573 2930 2931  916 2932  273 1783    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 2933   14   13   25  218 1787    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 2934  917   14   13  918 1788    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    7 2935  919 2936 2937 2938    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1  414 1789 2939    5    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 2940 1228    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  1 130 346  49   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   1 1790  920 2941  347    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   6  693   50 2942    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  1   3 348 574   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[  1  40   6 921  33   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   1    3  100  922   45 1791  101    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   4 1792  694  923    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1   81 2943  115   82    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   6  415  924 1793  481    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  19   28   82 2944 2945  575    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  1 195 925  55   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[   1 2946 1229    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    7  695   68  696 2947  697  576    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  19   42 1794  482   53 2948  926  246  173    5    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    7 1795 2949  296  347    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 1230  416    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  15 2950   58   56    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[  29  698 2951 1231 1796    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   8 1232   12 2952  417 1797    5   51  159    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 1798 2953  349   41  483  350    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    9 1799  351  927  699    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1  928 2954 2955 2956  352 2957   10 2958    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    7   24   11 1233  174    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2    7  700  353  219 1234  418 2959    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2   20 2960 2961 2962    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 1800  577   69 1801 1802 1235  143    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 1803  578   63 2963  131   57 2964 2965 2966    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1 2967  929  930    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   2  701 1804 2968  130    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[   1  931 2969   78 1805 2970    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# First of 50 records after padding to size 20\n",
    "for i in range(50):\n",
    "    print(train_padded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_questions)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC', 'NUM', 'HUM', 'ABBR', 'DESC', 'ENTY'}\n"
     ]
    }
   ],
   "source": [
    "# set of lables\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to token\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[1]\n",
      "[3]\n",
      "[1]\n",
      "[6]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[3]\n",
      "[2]\n",
      "[4]\n",
      "[3]\n",
      "[2]\n",
      "[2]\n",
      "[1]\n",
      "[5]\n",
      "[3]\n",
      "[4]\n",
      "[3]\n",
      "[3]\n",
      "[4]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[1]\n",
      "[2]\n",
      "[5]\n",
      "[1]\n",
      "[5]\n",
      "[6]\n",
      "[1]\n",
      "[4]\n",
      "[2]\n",
      "[4]\n",
      "[3]\n",
      "[4]\n",
      "[1]\n",
      "[5]\n",
      "[1]\n",
      "[3]\n",
      "[1]\n",
      "[4]\n",
      "[4]\n",
      "[3]\n",
      "[2]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# First of 50 labels (token form)\n",
    "for i in range(50):\n",
    "    print(training_label_seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "what date boxing day ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "What date Boxing Day ?\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Checking encode and original\n",
    "def decode_question(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print('------------------------')\n",
    "print(decode_question(train_padded[20]))\n",
    "print(train_questions[20])\n",
    "print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          192000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 266,759\n",
      "Trainable params: 266,759\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use tf.keras.layers.Bidirectional(tf.keras.layers.LSTM()).\n",
    "# Use ReLU in place of tanh function.\n",
    "# Add a Dense layer with 7 units and softmax activation.\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4361 samples, validate on 1091 samples\n",
      "Epoch 1/15\n",
      "4361/4361 - 7s - loss: 1.4846 - accuracy: 0.3843 - val_loss: 1.0704 - val_accuracy: 0.5683\n",
      "Epoch 2/15\n",
      "4361/4361 - 2s - loss: 0.6539 - accuracy: 0.7732 - val_loss: 0.5709 - val_accuracy: 0.7754\n",
      "Epoch 3/15\n",
      "4361/4361 - 2s - loss: 0.3043 - accuracy: 0.8989 - val_loss: 0.5829 - val_accuracy: 0.7782\n",
      "Epoch 4/15\n",
      "4361/4361 - 2s - loss: 0.1510 - accuracy: 0.9569 - val_loss: 0.6202 - val_accuracy: 0.7984\n",
      "Epoch 5/15\n",
      "4361/4361 - 2s - loss: 0.0947 - accuracy: 0.9768 - val_loss: 0.6201 - val_accuracy: 0.8139\n",
      "Epoch 6/15\n",
      "4361/4361 - 2s - loss: 0.0709 - accuracy: 0.9814 - val_loss: 0.6916 - val_accuracy: 0.8011\n",
      "Epoch 7/15\n",
      "4361/4361 - 2s - loss: 0.0611 - accuracy: 0.9846 - val_loss: 0.7962 - val_accuracy: 0.8020\n",
      "Epoch 8/15\n",
      "4361/4361 - 3s - loss: 0.0509 - accuracy: 0.9876 - val_loss: 0.7878 - val_accuracy: 0.8002\n",
      "Epoch 9/15\n",
      "4361/4361 - 3s - loss: 0.0447 - accuracy: 0.9892 - val_loss: 0.8889 - val_accuracy: 0.7993\n",
      "Epoch 10/15\n",
      "4361/4361 - 3s - loss: 0.0418 - accuracy: 0.9892 - val_loss: 0.8745 - val_accuracy: 0.7910\n",
      "Epoch 11/15\n",
      "4361/4361 - 2s - loss: 0.0384 - accuracy: 0.9915 - val_loss: 1.0111 - val_accuracy: 0.8011\n",
      "Epoch 12/15\n",
      "4361/4361 - 2s - loss: 0.0360 - accuracy: 0.9899 - val_loss: 1.0154 - val_accuracy: 0.7910\n",
      "Epoch 13/15\n",
      "4361/4361 - 2s - loss: 0.0378 - accuracy: 0.9901 - val_loss: 0.8786 - val_accuracy: 0.8066\n",
      "Epoch 14/15\n",
      "4361/4361 - 2s - loss: 0.0328 - accuracy: 0.9908 - val_loss: 0.9505 - val_accuracy: 0.8029\n",
      "Epoch 15/15\n",
      "4361/4361 - 3s - loss: 0.0304 - accuracy: 0.9913 - val_loss: 1.0182 - val_accuracy: 0.7974\n"
     ]
    }
   ],
   "source": [
    "# Traing model with 15 epochs\n",
    "num_epochs = 15\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2480495e-05 6.6372859e-03 1.4034165e-03 9.4354695e-01 3.0001828e-02\n",
      "  3.4741121e-03 1.4923892e-02]]\n",
      "ENTY\n"
     ]
    }
   ],
   "source": [
    "# Predict input text\n",
    "question_input = [\"What metal has the highest melting point ?\"]\n",
    "seq = tokenizer.texts_to_sequences(question_input)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "prediction = model.predict(padded)\n",
    "print(prediction)\n",
    "print(labels[np.argmax(prediction)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
